## （1）

参考：[残差网络ResNet解读(原创)](https://zhuanlan.zhihu.com/p/32702162)

何恺明提出了一种残差结构来实现上述恒等映射(图1)：整个模块除了正常的卷积层输出外，还有一个分支把输入直接连到输出上，该输出和卷积的输出做算术相加得到最终的输出，用公式表达就是H(x)=F(x)+x，x是输入，F(x)是卷积分支的输出，H(x)是整个结构的输出。可以证明如果F(x)分支中所有参数都是0，H(x)就是个恒等映射。残差结构人为制造了恒等映射，就能让整个结构朝着恒等映射的方向去收敛，确保最终的错误率不会因为深度的变大而越来越差。如果一个网络通过简单的手工设置参数值就能达到想要的结果，那这种结构就很容易通过训练来收敛到该结果，这是一条设计复杂的网络时百试不爽的规则。回想一下BN中为了在BN处理后恢复原有的分布，使用了y=rx+delta公式， 当手动设置r为标准差，delta为均值时，y就是BN处理前的分布，这就是利用了这条规则。

![](https://pic3.zhimg.com/80/v2-6d080f8db3d814c41d968a054132a44e_hd.jpg)



[深度学习——--残差网络(ResNet)](https://statusrank.xyz/2018/09/03/ResNet/)（大部分内容也都讲到了，不过这里提一点，大部分文章，关于参数的计算都少了算了偏置）

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20181228211241.png)

很明显，该网络是带有跳跃式结构的，残差网络借鉴了高速网络的跨层链接思想,但对其进行改进(残差项原本是带权值的，但是ResNet采用恒等映射代替)
假定某段神经网络的输入时x,期望输出是H(x),即H(x)是期望的复杂潜在映射,如果要是学习这样的模型,则训练的难度会比较大。回想我们前面的假设,如果学习到较饱和的准确率(或者发现下层的误差变大时),那么接下来的学习目标就会转变为恒等学习,也就是使输入x近似于输出H(x)，以保持在后面的层次中不会造成精度下降。
在上图的残差网络结构中,通过“shortcut connections(捷径连接)”的方式,直接把输入传到输出作为初始结果,使输出结果为H(x) = F(x) + x,当F(x) = 0,那么H(x) = x,也就是上面的恒等映射。于是,ResNet相当于将学习目标改变了,不再是学习一个完整的输出,而是目标值H(x)和x的差值,也就是所谓的残差**F(x) = H(x) - x**,因此后面的目标就是将残差结果逼近于0,使随着网络加深,准确率不下降。

关于残差连接的理解：[resnet中的残差连接，你确定真的看懂了？](https://zhuanlan.zhihu.com/p/42833949)

蒋竺波：[深度学习_残差网络 ResNet](http://i.youku.com/i/UNTU0NjkwNTQyNA==)，关于梯度弥散问题这里有讲解一番。

![](https://pic4.zhimg.com/80/v2-eb44512243a8e59c1fe02a7e6bd481db_hd.jpg)

---



## （2）





